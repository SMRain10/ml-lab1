{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 - Scikit-learn\n",
    "Author: Sam Rainbow\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The goal of this lab is to become familiar with the scikit-learn library.\n",
    "\n",
    "You will practice loading example datasets, perform classification and regression with linear scikit-learn models, and investigate the effects of reducing the number of features (columns in X) and the number of samples (rows in X and y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "Using yellowbrick spam - classification  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "The goal is to investigate `LogisticRegression(max_iter=2000)` and effects of reducing the number of features and number of samples on classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_classifier_accuracy(model, X, y):\n",
    "    '''Calculate train and validation accuracy of classifier (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn classifier): Classifier to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training accuracy, validation accuracy\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #TODO: IMPLEMENT FUNCTION BODY\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=956)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_test, y_test_pred)\n",
    "    return train_acc, val_acc   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_spam()`, load the spam data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print size and type of `X` and `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (4600, 57)  type:  <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y:  (4600,)  type:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from yellowbrick.datasets import load_spam\n",
    "\n",
    "X, y = load_spam()\n",
    "print(\"Size of X: \", X.shape, \" type: \", type(X))\n",
    "print(\"Size of y: \", y.shape, \" type: \", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (46, 57)  type:  <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y:  (46,)  type:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "X_small, _, y_small, _ = train_test_split(X,y, train_size = 0.01, random_state=174)\n",
    "print(\"Size of X: \", X_small.shape, \" type: \", type(X))\n",
    "print(\"Size of y: \", y_small.shape, \" type: \", type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and evaluate models\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "4. Call your convenience function `get_classifier_accuracy()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation accuracy for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Data Size  Training Accuracy  Validation Accuracy\n",
      "0          X and y           0.935072             0.917391\n",
      "1  First 2 columns           0.608986             0.613043\n",
      "2    X and y small           0.941176             0.750000\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter =2000)\n",
    "\n",
    "col = [\"Data Size\", \"Training Accuracy\", \"Validation Accuracy\"]\n",
    "results = pd.DataFrame(columns = col)\n",
    "\n",
    "t_acc, v_acc = get_classifier_accuracy(model, X, y)\n",
    "results.loc[0] = [\"X and y\",t_acc, v_acc]\n",
    "t_acc, v_acc = get_classifier_accuracy(model, X.iloc[:,:2], y)\n",
    "results.loc[1] = [\"First 2 columns\",t_acc, v_acc]\n",
    "t_acc, v_acc = get_classifier_accuracy(model, X_small, y_small)\n",
    "results.loc[2] = [\"X and y small\",t_acc, v_acc]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Questions\n",
    "1. What is the validation accuracy using all data? What is the difference between training and validation accuracy?\n",
    "2. How does the validation accuracy and difference between training and validation change when only two columns are used? Provide values.\n",
    "3. How does the validation accuracy and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "1. The validation accuracy using all of the data is 0.917, with a difference of 0.018 between the training and validation accuracy. This indicates that model is accurately predicting unseen data.\n",
    "2. When only two columns are used the validation accuracy decreases substantially. The difference between the training accuracy (0.609) and validation accuracy (0.613) is 0.0042 which even lower than the full set. Using only 2 columns likely results in a less\n",
    "complex model due reduced data. \n",
    "3. When the small data sets are used the validation accuracy decreases substantially, but the training accuracy increases. The difference between the training accuracy (0.941) and validation accuracy (0.75) is 0.191 which is higher than the other two scenarios. This indicates overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "\n",
    "Using yellowbrick energy - regression  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/energy.html\n",
    "\n",
    "The goal is to investigate `LinearRegression()` and effects of reducing the number of features and number of samples on regression performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_regressor_mse(model, X, y):\n",
    "    '''Calculate train and validation mean-squared error (mse) of regressor (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn regressor): Regressor to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training mse, validation mse\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    #TODO: IMPLEMENT FUNCTION BODY\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=956)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    train_mse = mean_squared_error(y_train,y_train_predict)\n",
    "    val_mse = mean_squared_error(y_test, y_test_predict)\n",
    "    return train_mse, val_mse   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_energy()` load the energy data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print dimensions and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (768, 8)  type:  <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y:  (768,)  type:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from yellowbrick.datasets import load_energy\n",
    "\n",
    "\n",
    "X, y = load_energy()\n",
    "print(\"Size of X: \", X.shape, \" type: \", type(X))\n",
    "print(\"Size of y: \", y.shape, \" type: \", type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (7, 8)  type:  <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y:  (7,)  type:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "X_small, _, y_small, _ = train_test_split(X,y, train_size = 0.01, random_state=174)\n",
    "print(\"Size of X: \", X_small.shape, \" type: \", type(X))\n",
    "print(\"Size of y: \", y_small.shape, \" type: \", type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train and evaluate models\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training MSE, validation MSE\n",
    "4. Call your convenience function `get_regressor_mse()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation MSE for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Data Size  Training MSE  Validation MSE\n",
      "0          X and y  7.981975e+00       10.292306\n",
      "1  First 2 columns  5.360043e+01       46.410426\n",
      "2    X and y small  2.284541e-28       69.977449\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "col = [\"Data Size\", \"Training MSE\", \"Validation MSE\"]\n",
    "results2 = pd.DataFrame(columns = col)\n",
    "\n",
    "t_mse, v_mse = get_regressor_mse(model, X, y)\n",
    "results2.loc[0] = [\"X and y\",t_mse, v_mse]\n",
    "t_mse, v_mse = get_regressor_mse(model, X.iloc[:,:2], y)\n",
    "results2.loc[1] = [\"First 2 columns\",t_mse, v_mse]\n",
    "t_mse, v_mse = get_regressor_mse(model, X_small, y_small)\n",
    "results2.loc[2] = [\"X and y small\",t_mse, v_mse]\n",
    "\n",
    "print(results2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Questions\n",
    "1. What is the validation MSE using all data? What is the difference between training and validation MSE?\n",
    "2. How does the validation MSE and difference between training and validation change when only two columns are used? Provide values.\n",
    "3. How does the validation MSE and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. The validation mse using all of the data is 10.29, with a difference of 2.31 between the training and validation mse. The difference between the MSE is small which indicates that model is accurately predicting unseen data.\n",
    "2. When only two columns are used the validation MSE increases substantially. The difference between the training MSE (53.6) and validation MSE (46.4) is 7.19 which is higher than the full set. Using only 2 columns likely results in a less\n",
    "complex model due reduced data. \n",
    "3. When the small data sets are used the validation MSE increases substantially, but the training MSE decreases to 0. The difference between the training MSE (0) and validation accuracy (69.97) is 69.97 which is higher than both other scenarios. This indicates overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observations/Interpretation\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "Across both the classification and regression data sets:\n",
    "1. We see that when using the full data sets the training and validation metrics both indicate a good fit of the data. The validation accuracy(0.917) and training accuracy(0.935) were both close and high, indication a good model fit (appropriate complexity, low bias and variance) while in the regression the MSE's were low and close together. The model is not overfitting or underfitting the data.\n",
    "\n",
    "2. In the second scenario when we reduce the number of features to 2 the training accuracy (0.609) and validation accuracy (0.613) were both low, and the training MSE (53.6) and validation MSE (46.4) are both very high and have a large difference. This is likely due\n",
    "to the reduced data and complexity of the model since there are only 2 features. Likely the model is underfitting the data leading to high bias and low variance as the model is making consistent innaccurate predictions.\n",
    "\n",
    "3. In the third scenario we only use 1% of the data. The training accuracy (0.94) and validation accuracy (0.75) had a large difference between the training and validation accuracies. The training MSE (0) and validation MSE (69.98) also had a very large difference.  In both scenarios the model was likely overfitting the data since there was not enough data. This would indicate that the model is overfitting the training data, resulting in high variance and low bias which is caused when a model is too complex for the amount of data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflection\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "It was interesting to get to see appropriately fitted data, underfit data, and overfit data just by manipulating the amount of data or features. We have learned the theory of this in class but seeing the three fits side by side is a great way to learn\n",
    "how the data causes good or poor models.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c97521948b64ea5cd22154effa14a99dc236ccb0c4ec0ab2858c53feb37c87d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
